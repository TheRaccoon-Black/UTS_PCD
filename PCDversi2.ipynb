{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CDWnLSSdr255"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply CLAHE to an X-ray image\n",
        "def apply_clahe(image):\n",
        "    # Check if image is already in grayscale\n",
        "    if len(image.shape) == 3:  # If the image has 3 channels (RGB)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
        "\n",
        "    # Ensure the image is in uint8 format\n",
        "    if image.dtype != 'uint8':\n",
        "        image = (image * 255).clip(0, 255).astype('uint8')  # Convert float image to uint8 if necessary\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    enhanced_image = clahe.apply(image)  # Apply CLAHE to the grayscale image\n",
        "    return enhanced_image\n",
        "\n",
        "# #Function to preprocess images with CLAHE\n",
        "# def preprocess_image(image):\n",
        "#     if isinstance(image, np.ndarray):\n",
        "#         enhanced_image = apply_clahe(image)\n",
        "#         enhanced_image = cv2.cvtColor(enhanced_image, cv2.COLOR_GRAY2RGB)  # Convert back to RGB\n",
        "#         return enhanced_image\n",
        "#     return image  # If the input is not an ndarray, return it as is\n",
        "\n",
        "# Function to preprocess images with CLAHE\n",
        "def preprocess_image(image):\n",
        "    if isinstance(image, np.ndarray):\n",
        "        enhanced_image = apply_clahe(image)\n",
        "        enhanced_image = cv2.cvtColor(enhanced_image, cv2.COLOR_GRAY2RGB)  # Convert back to RGB\n",
        "        # Ensure the final image is float32 for compatibility with ImageDataGenerator\n",
        "        return enhanced_image.astype('float32') / 255.0  # Scale to [0, 1]\n",
        "    return image  # If the input is not an ndarray, return it as is\n",
        "\n",
        "\n",
        "# Function to create ImageDataGenerator with preprocessing and optional validation split\n",
        "def create_data_generator(preprocess_func, validation_split=None):\n",
        "    return ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_func,\n",
        "        rescale=1./255,  # Scale the pixel values to [0, 1]\n",
        "        validation_split=validation_split  # Split for validation\n",
        "    )\n",
        "\n",
        "# Function to build and compile the model\n",
        "def build_model():\n",
        "    base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_layer = Dense(1, activation='sigmoid')(x)  # Binary classification output\n",
        "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate_model(preprocess_func, model_name):\n",
        "    print(f\"Training model with preprocessing: {model_name}\")\n",
        "\n",
        "    train_datagen = create_data_generator(preprocess_func, validation_split=0.2)  # Split 20% for validation\n",
        "    test_datagen = create_data_generator(None)\n",
        "\n",
        "    # Training generator with validation split\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dataset_path,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        subset='training'  # Use the training subset\n",
        "    )\n",
        "\n",
        "    val_generator = train_datagen.flow_from_directory(\n",
        "        train_dataset_path,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        subset='validation'  # Use the validation subset\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dataset_path,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "        epochs=10  # Set the desired number of epochs\n",
        "    )\n",
        "\n",
        "    model.save(f'/content/drive/MyDrive/densenet201_{model_name.lower().replace(\" \", \"_\")}_classifier.h5')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "    print(f\"Test accuracy for {model_name}: {test_acc}\")\n",
        "    return history, test_acc\n"
      ],
      "metadata": {
        "id": "ernrUPOVr8O4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVn6QYS0sAiU",
        "outputId": "b7c470f8-2ca4-41d5-b8dd-0454a46ac7cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_path = '/content/drive/MyDrive/PCD/archive/chest_xray/chest_xray/train'\n",
        "# test_dataset_path = '/content/drive/MyDrive/PCD/archive/chest_xray/chest_xray/test'\n",
        "\n",
        "# Define dataset paths\n",
        "train_dataset_path = '/content/drive/MyDrive/PCD/archive/chest_xray/train'\n",
        "test_dataset_path = '/content/drive/MyDrive/PCD/archive/chest_xray/test'\n",
        "val_dataset_path = '/content/drive/MyDrive/PCD/archive/chest_xray/val'"
      ],
      "metadata": {
        "id": "SyYLaMA-ugba"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model with CLAHE\n",
        "history_with_clahe, test_acc_with_clahe = train_and_evaluate_model(preprocess_image, \"With CLAHE\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGSpE-9G1XhV",
        "outputId": "20259455-efdd-4b5e-9217-008e5c21b610"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with preprocessing: With CLAHE\n",
            "Found 4180 images belonging to 2 classes.\n",
            "Found 1044 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 4s/step - accuracy: 0.7182 - loss: 0.5973 - val_accuracy: 0.7432 - val_loss: 0.5697\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/130\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - accuracy: 0.6875 - loss: 0.6156"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 91ms/step - accuracy: 0.6875 - loss: 0.6156 - val_accuracy: 0.7000 - val_loss: 0.6056\n",
            "Epoch 3/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 557ms/step - accuracy: 0.7426 - loss: 0.5661 - val_accuracy: 0.7432 - val_loss: 0.5629\n",
            "Epoch 4/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7812 - loss: 0.5260 - val_accuracy: 0.7000 - val_loss: 0.6038\n",
            "Epoch 5/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 552ms/step - accuracy: 0.7419 - loss: 0.5623 - val_accuracy: 0.7432 - val_loss: 0.5583\n",
            "Epoch 6/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7188 - loss: 0.5799 - val_accuracy: 0.7000 - val_loss: 0.5991\n",
            "Epoch 7/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 568ms/step - accuracy: 0.7386 - loss: 0.5609 - val_accuracy: 0.7422 - val_loss: 0.5555\n",
            "Epoch 8/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.6271 - val_accuracy: 0.7500 - val_loss: 0.5518\n",
            "Epoch 9/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 558ms/step - accuracy: 0.7317 - loss: 0.5634 - val_accuracy: 0.7451 - val_loss: 0.5502\n",
            "Epoch 10/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7188 - loss: 0.5776 - val_accuracy: 0.6000 - val_loss: 0.6737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 10s/step - accuracy: 0.4355 - loss: 0.8323\n",
            "Test accuracy for With CLAHE: 0.4440789520740509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate model without CLAHE\n",
        "history_without_clahe, test_acc_without_clahe = train_and_evaluate_model(None, \"No CLAHE\")"
      ],
      "metadata": {
        "id": "vsO4mafe1X6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a667a3-9c71-4669-9ef8-2dd61bdcdd5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with preprocessing: No CLAHE\n",
            "Found 4180 images belonging to 2 classes.\n",
            "Found 1044 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 694ms/step - accuracy: 0.8336 - loss: 0.3777 - val_accuracy: 0.9561 - val_loss: 0.1589\n",
            "Epoch 2/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1089 - val_accuracy: 1.0000 - val_loss: 0.0990\n",
            "Epoch 3/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 514ms/step - accuracy: 0.9371 - loss: 0.1625 - val_accuracy: 0.9629 - val_loss: 0.1227\n",
            "Epoch 4/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0474 - val_accuracy: 1.0000 - val_loss: 0.0706\n",
            "Epoch 5/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 519ms/step - accuracy: 0.9543 - loss: 0.1267 - val_accuracy: 0.9648 - val_loss: 0.1044\n",
            "Epoch 6/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.9688 - loss: 0.0919 - val_accuracy: 1.0000 - val_loss: 0.0571\n",
            "Epoch 7/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 508ms/step - accuracy: 0.9565 - loss: 0.1092 - val_accuracy: 0.9707 - val_loss: 0.0925\n",
            "Epoch 8/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1700 - val_accuracy: 1.0000 - val_loss: 0.0471\n",
            "Epoch 9/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 500ms/step - accuracy: 0.9652 - loss: 0.0959 - val_accuracy: 0.9697 - val_loss: 0.0856\n",
            "Epoch 10/10\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.9688 - loss: 0.0725 - val_accuracy: 1.0000 - val_loss: 0.0667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 436ms/step - accuracy: 0.7766 - loss: 0.5414\n",
            "Test accuracy for No CLAHE: 0.7680920958518982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4jfKAmI4WvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}